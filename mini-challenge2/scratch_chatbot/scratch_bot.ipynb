{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    npr Mini-Challenge 2 <br> \n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Si Ben Tran, Yannic Lais, Rami Tarabishi im HS 2023.<br>Bachelor of Science FHNW in Data Science.\n",
    "</p>\n",
    "\n",
    "\n",
    "# Aufgabe aus dem Space\n",
    "Mini-Challenge 2-C (LE6):\n",
    "Chatbot\n",
    "\n",
    "You build a simple chatbot for hotel recommendation (here you can combine with npr and webscraping challenge) or extend the chatbot from the library (auxilio, please take contact with me), with rasa or dialogflow. You can also propose a use case. Important is the way intent are recognized and NER or variables are extracted. Delivery is a report on the use case, interactions and how intents are recognized and variables extracted. Also an error analysis on concrete conversations (confidence of models, explanation of predictions, etc.) should be undertaken. The submission can be done in a group of 2 or 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from preprocessing import tokenize, stem, bag_of_words\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['believe', 'believed', 'believes', 'believing', 'believer']\n",
      "['believ', 'believ', 'believ', 'believ', 'believ']\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"believe\", \"believed\", \"believes\", \"believing\", \"believer\"]\n",
    "print(word_list)\n",
    "stemmed_words = [stem(w) for w in word_list]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Test Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['believe', 'believed', 'believes', 'believing', 'believer']\n",
      "['believ', 'believ', 'believ', 'believ', 'believ']\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"believe\", \"believed\", \"believes\", \"believing\", \"believer\"]\n",
    "print(word_list)\n",
    "stemmed_words = [stem(w) for w in word_list]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "bag = bag_of_words(sentence, words)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Hey', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up', 'How', 'are', 'you', 'doing', 'Howdy', 'Hi', 'there', 'Hola', 'Good', 'morning', 'Good', 'afternoon', 'Good', 'evening', 'It', 'is', 'nice', 'to', 'meet', 'you', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Nice', 'chatting', 'to', 'you', ',', 'bye', 'Till', 'next', 'time', 'Have', 'a', 'nice', 'day', 'See', 'you', '!', 'Later', '!', 'Goodbye', '!', 'I', \"'m\", 'off', '!', 'That', \"'s\", 'all', 'for', 'now', '.', 'See', 'you', 'soon', '!', 'I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Bye', '!', 'It', 'was', 'nice', 'talking', 'to', 'you', '.', 'See', 'you', 'soon', '.', 'I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Have', 'a', 'great', 'day', '!', 'It', 'was', 'nice', 'talking', 'to', 'you', '.', 'Have', 'a', 'great', 'day', '!', 'I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Have', 'a', 'nice', 'day', '!', 'It', 'was', 'nice', 'talking', 'to', 'you', '.', 'Have', 'a', 'nice', 'day', '!', 'I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'See', 'you', 'later', '!', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'Thank', \"'s\", 'a', 'lot', '!', 'Thanks', 'for', 'helping', 'me', 'Awesome', ',', 'thanks', 'Thanks', 'for', 'the', 'help', 'Thanks', 'a', 'bunch', 'Thanks', 'for', 'everything', 'Thanks', 'for', 'the', 'chat', 'Thanks', 'for', 'the', 'conversation', 'Thanks', 'for', 'the', 'info', 'Which', 'items', 'do', 'you', 'have', '?', 'What', 'kinds', 'of', 'items', 'are', 'there', '?', 'What', 'do', 'you', 'sell', '?', 'What', 'are', 'your', 'items', '?', 'What', 'do', 'you', 'have', '?', 'What', 'are', 'you', 'selling', '?', 'What', 'do', 'you', 'offer', '?', 'What', 'do', 'you', 'have', 'for', 'sale', '?', 'What', 'do', 'you', 'have', 'in', 'stock', '?', 'What', 'are', 'your', 'products', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Can', 'I', 'pay', 'with', 'Paypal', '?', 'Are', 'you', 'cash', 'only', '?', 'Do', 'you', 'accept', 'VISA', '?', 'Do', 'you', 'accept', 'AMEX', '?', 'Do', 'you', 'accept', 'Discover', 'Card', '?', 'Do', 'you', 'accept', 'Apple', 'Pay', '?', 'Do', 'you', 'accept', 'Google', 'Pay', '?', 'Do', 'you', 'accept', 'Samsung', 'Pay', '?', 'Do', 'you', 'accept', 'Bitcoin', '?', 'Do', 'you', 'accept', 'cryptocurrency', '?', 'Do', 'you', 'accept', 'crypto', '?', 'Do', 'you', 'accept', 'Dogecoin', '?', 'Do', 'you', 'accept', 'Ethereum', '?', 'How', 'long', 'does', 'delivery', 'take', '?', 'How', 'long', 'does', 'shipping', 'take', '?', 'When', 'do', 'I', 'get', 'my', 'delivery', '?', 'When', 'do', 'I', 'get', 'my', 'order', '?', 'When', 'do', 'I', 'get', 'my', 'package', '?', 'When', 'do', 'I', 'get', 'my', 'shipment', '?', 'When', 'do', 'I', 'get', 'my', 'item', '?', 'Tell', 'me', 'a', 'joke', '!', 'Tell', 'me', 'something', 'funny', '!', 'Do', 'you', 'know', 'a', 'joke', '?', 'Do', 'you', 'know', 'any', 'jokes', '?', 'Do', 'you', 'know', 'any', 'funny', 'jokes', '?']\n",
      "['greeting', 'goodbye', 'thanks', 'items', 'payments', 'delivery', 'funny']\n",
      "[(['Hi'], 'greeting'), (['Hey'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Whats', 'up'], 'greeting'), (['How', 'are', 'you', 'doing'], 'greeting'), (['Howdy'], 'greeting'), (['Hi', 'there'], 'greeting'), (['Hola'], 'greeting'), (['Good', 'morning'], 'greeting'), (['Good', 'afternoon'], 'greeting'), (['Good', 'evening'], 'greeting'), (['It', 'is', 'nice', 'to', 'meet', 'you'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Have', 'a', 'nice', 'day'], 'goodbye'), (['See', 'you', '!'], 'goodbye'), (['Later', '!'], 'goodbye'), (['Goodbye', '!'], 'goodbye'), (['I', \"'m\", 'off', '!'], 'goodbye'), (['That', \"'s\", 'all', 'for', 'now', '.', 'See', 'you', 'soon', '!'], 'goodbye'), (['I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Bye', '!'], 'goodbye'), (['It', 'was', 'nice', 'talking', 'to', 'you', '.', 'See', 'you', 'soon', '.'], 'goodbye'), (['I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Have', 'a', 'great', 'day', '!'], 'goodbye'), (['It', 'was', 'nice', 'talking', 'to', 'you', '.', 'Have', 'a', 'great', 'day', '!'], 'goodbye'), (['I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'Have', 'a', 'nice', 'day', '!'], 'goodbye'), (['It', 'was', 'nice', 'talking', 'to', 'you', '.', 'Have', 'a', 'nice', 'day', '!'], 'goodbye'), (['I', \"'ve\", 'got', 'to', 'get', 'going', '.', 'See', 'you', 'later', '!'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'the', 'help'], 'thanks'), (['Thanks', 'a', 'bunch'], 'thanks'), (['Thanks', 'for', 'everything'], 'thanks'), (['Thanks', 'for', 'the', 'chat'], 'thanks'), (['Thanks', 'for', 'the', 'conversation'], 'thanks'), (['Thanks', 'for', 'the', 'info'], 'thanks'), (['Which', 'items', 'do', 'you', 'have', '?'], 'items'), (['What', 'kinds', 'of', 'items', 'are', 'there', '?'], 'items'), (['What', 'do', 'you', 'sell', '?'], 'items'), (['What', 'are', 'your', 'items', '?'], 'items'), (['What', 'do', 'you', 'have', '?'], 'items'), (['What', 'are', 'you', 'selling', '?'], 'items'), (['What', 'do', 'you', 'offer', '?'], 'items'), (['What', 'do', 'you', 'have', 'for', 'sale', '?'], 'items'), (['What', 'do', 'you', 'have', 'in', 'stock', '?'], 'items'), (['What', 'are', 'your', 'products', '?'], 'items'), (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'), (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'), (['Can', 'I', 'pay', 'with', 'Paypal', '?'], 'payments'), (['Are', 'you', 'cash', 'only', '?'], 'payments'), (['Do', 'you', 'accept', 'VISA', '?'], 'payments'), (['Do', 'you', 'accept', 'AMEX', '?'], 'payments'), (['Do', 'you', 'accept', 'Discover', 'Card', '?'], 'payments'), (['Do', 'you', 'accept', 'Apple', 'Pay', '?'], 'payments'), (['Do', 'you', 'accept', 'Google', 'Pay', '?'], 'payments'), (['Do', 'you', 'accept', 'Samsung', 'Pay', '?'], 'payments'), (['Do', 'you', 'accept', 'Bitcoin', '?'], 'payments'), (['Do', 'you', 'accept', 'cryptocurrency', '?'], 'payments'), (['Do', 'you', 'accept', 'crypto', '?'], 'payments'), (['Do', 'you', 'accept', 'Dogecoin', '?'], 'payments'), (['Do', 'you', 'accept', 'Ethereum', '?'], 'payments'), (['How', 'long', 'does', 'delivery', 'take', '?'], 'delivery'), (['How', 'long', 'does', 'shipping', 'take', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'delivery', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'order', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'package', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'shipment', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'item', '?'], 'delivery'), (['Tell', 'me', 'a', 'joke', '!'], 'funny'), (['Tell', 'me', 'something', 'funny', '!'], 'funny'), (['Do', 'you', 'know', 'a', 'joke', '?'], 'funny'), (['Do', 'you', 'know', 'any', 'jokes', '?'], 'funny'), (['Do', 'you', 'know', 'any', 'funny', 'jokes', '?'], 'funny')]\n"
     ]
    }
   ],
   "source": [
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    # add to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # add to our words list\n",
    "        all_words.extend(w)\n",
    "        # add to xy pair\n",
    "        xy.append((w, tag))\n",
    "\n",
    "print(all_words)\n",
    "print(tags)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Preprocessing Intent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 patterns\n",
      "7 tags: ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n",
      "107 unique stemmed words: [\"'m\", \"'s\", \"'ve\", 'a', 'accept', 'afternoon', 'all', 'amex', 'ani', 'anyon', 'appl', 'are', 'awesom', 'bitcoin', 'bunch', 'bye', 'can', 'card', 'cash', 'chat', 'convers', 'credit', 'crypto', 'cryptocurr', 'day', 'deliveri', 'discov', 'do', 'doe', 'dogecoin', 'ethereum', 'even', 'everyth', 'for', 'funni', 'get', 'go', 'good', 'goodby', 'googl', 'got', 'great', 'have', 'hello', 'help', 'hey', 'hi', 'hola', 'how', 'howdi', 'i', 'in', 'info', 'is', 'it', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'meet', 'morn', 'my', 'next', 'nice', 'now', 'of', 'off', 'offer', 'onli', 'order', 'packag', 'pay', 'paypal', 'product', 'sale', 'samsung', 'see', 'sell', 'ship', 'shipment', 'someth', 'soon', 'stock', 'take', 'talk', 'tell', 'thank', 'that', 'the', 'there', 'till', 'time', 'to', 'up', 'visa', 'wa', 'what', 'when', 'which', 'with', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# stem and lower each word and remove duplicates and sort\n",
    "ignore_words = [\"?\", \"!\", \".\", \",\"]\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 107)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "(82,)\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for pattern_sentence, tag in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "tensor([2, 2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# define Batch size\n",
    "batch_size = 2\n",
    "\n",
    "dataset = ChatDataset(X_train=X_train, y_train=y_train)\n",
    "\n",
    "print(dataset.n_samples)\n",
    "print(dataset.x_data)\n",
    "print(dataset.y_data)\n",
    "\n",
    "# create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "# get first batch\n",
    "for first_batch in train_loader:\n",
    "    break\n",
    "# get first training batch\n",
    "first_batch_inputs = first_batch[0]\n",
    "first_batch_labels = first_batch[1]\n",
    "print(first_batch_inputs)\n",
    "print(first_batch_labels)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 7\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = batch_size\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Model Instanzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(neural_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [00:07<00:00, 12.71epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0009\n",
      "final loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    # Wrap the train_loader with tqdm to create a progress bar\n",
    "    for words, labels in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_net(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:  # Print loss after every 10 epochs\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "print(f\"final loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model_state\": neural_net.state_dict(),\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"output_size\": output_size,\n",
    "    \"all_words\": all_words,\n",
    "    \"tags\": tags,\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f\"training complete. file saved to {FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! type 'quit' to exit\n",
      "Bounty Hunter: I do not understand...\n",
      "Bounty Hunter: I do not understand...\n",
      "Bounty Hunter: We sell coffee and tea\n",
      "Bounty Hunter: Hello, how can I help you?\n",
      "Bounty Hunter: I do not understand...\n",
      "Bounty Hunter: I do not understand...\n",
      "Bounty Hunter: We have a wide range of coffee and tea\n",
      "Bounty Hunter: I do not understand...\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data[\"all_words\"]\n",
    "tags = data[\"tags\"]\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "neural_net = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "neural_net.load_state_dict(model_state)\n",
    "neural_net.eval()\n",
    "\n",
    "bot_name = \"Stupid Bounty Hunter Bot\"\n",
    "print(\"Let's chat! type 'quit' to exit\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = neural_net(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.5:\n",
    "        for intent in intents[\"intents\"]:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {np.random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratchbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
